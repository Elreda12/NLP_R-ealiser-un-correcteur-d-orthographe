{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a629416",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fra_mixed-typical_2012_1M-words.txt\", 'r', encoding='utf-8') as f:\n",
    "    vocabulaire = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68de92f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '^', '_', '€', '{', '}', '~', '`', '´', '§', '¨', '©', '«', '®', '°', '»', '¿', '×', '¤', '£', '¥', '“', '”', '„', '¢', 'de', 'est', 'la', 'le', 'à']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['向井優', '大島良明', '山东航空', '心', '敗戰計', '文', '新井優', '村松修', '東京（日本国）', '森弘', '無', '王原祁', '而已', '谷', '金井清高', '鈴木憲蔵', '除了這些特別環節之外，電影節也介紹多個紀錄片節目，而其中的〝親愛的地球〞是集中探討地球的狀況，其中一部出色的紀錄片是歌連謝荷導演的《自耕救地球》(Solutions', '隆', '\\ue04a', '\\ue057', '\\ue40d', '\\ue410', '\\ue6e5', '\\uf02d', '\\uf0a7', '\\uf0b7', '\\uf0d8', '\\uf732\\uf730\\uf730', '：', 'ｽsentent', '�lectrique', '�quipement', '�tage', '�tre', '�tudiants', '�t�', '�volue', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...']\n"
     ]
    }
   ],
   "source": [
    "voca = []\n",
    "\n",
    "# parcourir chaque ligne dans le texte\n",
    "for ligne in vocabulaire:\n",
    "    # diviser la ligne en trois parties et stocker la deuxième partie\n",
    "    deuxieme_partie = ligne.split()[1]\n",
    "    # ajouter la deuxième partie à la liste\n",
    "    voca.append(deuxieme_partie)\n",
    "\n",
    "# afficher la liste des deuxièmes parties\n",
    "\n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voca[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voca[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd40986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad7bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_pos import get_word_tag, preprocess  \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d783cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voca = list(set(voca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f45ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae9660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juin-août:0\n",
      "aborder:1\n",
      "L'allocution:2\n",
      "rentre-t-il:3\n",
      "Barjavel:4\n",
      "Koko:5\n",
      "endroit.:6\n",
      "Béjaïa:7\n",
      "jouréne:8\n",
      "Cavités:9\n",
      "L'Etna:10\n",
      "polyploïdes:11\n",
      "ARTHUR:12\n",
      "Médecin-major:13\n",
      "universelle.:14\n",
      "Altona:15\n",
      "d3o:16\n",
      "colonisent:17\n",
      "Belle:18\n",
      "TARGET:19\n",
      "smilies:20\n",
      "Malacca:21\n",
      "m'atteignaient:22\n",
      "d'individualisme:23\n",
      "cocktail-bénéfice:24\n",
      "IDENA:25\n",
      "encadreurs:26\n",
      "adjuvants:27\n",
      "Kudrin:28\n",
      "l'accepter:29\n",
      "umrinvit:30\n",
      "parles-en:31\n",
      "L'assimilation:32\n",
      "16H16:33\n",
      "RESPECT:34\n",
      "Carences:35\n",
      "Lanka:36\n",
      "galérien:37\n",
      "Apologie:38\n",
      "homeshoring:39\n",
      "traceurs:40\n",
      "régional.:41\n",
      "Adventice:42\n",
      "exclura:43\n",
      "forts.:44\n",
      "groupe-individu:45\n",
      "metalheads:46\n",
      "fanboy:47\n",
      "6,8:48\n",
      "SGANARELLE:49\n",
      "crée:50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {} \n",
    " \n",
    "for i in range(0,len(voca)): \n",
    "    vocab[voca[i]] = i\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c25bdd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a804d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      "!:0\n",
      "\":1\n",
      "#:2\n",
      "$:3\n",
      "$.:4\n",
      "$/personne:5\n",
      "$1:6\n",
      "$1000:7\n",
      "$20:8\n",
      "$400:9\n",
      "$500:10\n",
      "$60:11\n",
      "$600:12\n",
      "$800k:13\n",
      "$?:14\n",
      "$content:15\n",
      "$contents:16\n",
      "$lambda$-termes:17\n",
      "$n:18\n",
      "$postpone:19\n",
      "$t:20\n",
      "%:21\n",
      "%!:22\n",
      "%.:23\n",
      "&:24\n",
      "':25\n",
      "(:26\n",
      "):27\n",
      "*:28\n",
      "+:29\n",
      ",:30\n",
      "-:31\n",
      "--:32\n",
      "--n--:33\n",
      "--unk--:34\n",
      "--unk_adj--:35\n",
      "--unk_adv--:36\n",
      "--unk_digit--:37\n",
      "--unk_noun--:38\n",
      "--unk_punct--:39\n",
      "--unk_upper--:40\n",
      "--unk_verb--:41\n",
      ".:42\n",
      "...:43\n",
      "/:44\n",
      "0:45\n",
      "0%:46\n",
      "0''19:47\n",
      "0''7:48\n",
      "0'100:49\n",
      "0,000:50\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {} \n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voca)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931cc33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['Nutashkuan_NPP', 'se_CLR', 'sont_V', 'retirées_VPP', 'des_DET', 'négociations_NC', 'territoriales_ADJ', 'avec_P', 'le_DET', 'gouvernement_NC']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "\n",
    "with open(\"tags_train.txt\", 'r', encoding='utf-8') as f:\n",
    "    training = f.read()\n",
    "    training_corpus = training.split()\n",
    "    \n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f966c483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2028032"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bbe4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_corpus = training_corpus[0:33000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ee2df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf55c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268123"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1fca254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['Ce_DET', 'vendredi_NC', ',_PONCT', 'quatre_DET', 'matchs_NC', 'de_P', 'la_DET', 'ligue_NC', 'nationale_ADJ', 'de_P']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"tags_test.txt\", 'r', encoding='utf-8') as f:\n",
    "    test = f.read()\n",
    "    y = test.split()\n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e3303ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  507364\n",
      "This is a sample of the test_corpus: \n",
      "['éventuel', '--unk--', 'ou', 'à', 'des', 'escroqueries', '.', 'Bien', 'que', 'Barack', 'Obama', 'ait', 'obtenu', '61', '%', 'des', 'suffrages', 'en', 'ayant', 'rallié', '--unk_punct--', 'appui', 'de', 'la', 'communauté', 'noire', ',', 'Hillary', 'Clinton', 'a', 'rassemblé', '37', '%', 'des', 'votes', 'pour', 'les', '33', 'délégués', 'de', '--unk_punct--', 'état', '.', 'Ce', 'genre', '--unk_punct--', 'action', 'est', 'inédite', 'dans']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"words.txt\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ee9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "# Punctuation characters\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Morphology rules used to assign unknown word tokens\n",
    "noun_suffix = [\"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0dd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens\n",
    "    \"\"\"\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9722a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag1(line, vocab):\n",
    "    if not line.strip():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "    else:\n",
    "        a = line.split(\"_\")\n",
    "        if len(a) == 2:\n",
    "            word, tag = a\n",
    "            if word not in vocab: \n",
    "                word = assign_unk(word)\n",
    "        else:\n",
    "            word = \"--n--\"\n",
    "            tag = \"--s--\"\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "838ecb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('se', 'CLR')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag1('se_CLR', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fa59202",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for word_tag in training_corpus:\n",
    "    a = get_word_tag1(word_tag, vocab)\n",
    "    l.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e6fb58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communautés', 'NC'),\n",
       " ('--unk--', 'ADJ'),\n",
       " ('--unk_punct--', 'P'),\n",
       " ('--unk_upper--', 'NPP')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c152939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_dictionaries\n",
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"word count = {i}\")\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        word, tag = get_word_tag1(word_tag,vocab) \n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c89b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bcaadc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 30\n",
      "View these POS tags (states)\n",
      "['--s--', 'ADJ', 'ADJWH', 'ADV', 'ADVWH', 'CC', 'CLO', 'CLR', 'CLS', 'CS', 'DET', 'DETWH', 'ET', 'I', 'NC', 'NPP', 'P', 'P+D', 'PONCT', 'PREF', 'PRO', 'PROREL', 'PROWH', 'U', 'V', 'VIMP', 'VINF', 'VPP', 'VPR', 'VS']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9454eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'DET'), 1)\n",
      "(('DET', 'NC'), 3895)\n",
      "(('NC', 'ADJ'), 1127)\n",
      "\n",
      "emission examples: \n",
      "(('NC', 'services'), 9)\n",
      "(('NC', 'complément'), 1)\n",
      "(('NC', 'fusée'), 2)\n",
      "\n",
      "ambiguous word example: \n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[150:153]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0ebb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
    "    total = len(y)\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        \n",
    "        \n",
    "\n",
    "        # Split the (word, POS) string into a list of two items\n",
    "        y_tup_l = y_tup.split('_')\n",
    "        \n",
    "        # Verify that y_tup contain both word and POS\n",
    "        if len(y_tup_l) == 2:\n",
    "            \n",
    "            # Set the true POS label for this word\n",
    "            true_label = y_tup_l[1]\n",
    "\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word print(3)\n",
    "            continue\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        # If the word is in the vocabulary...\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "\n",
    "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "                        \n",
    "                # define the key as the tuple containing the POS and word\n",
    "                key = (pos,word)\n",
    "\n",
    "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
    "                if key in emission_counts: # complete this line\n",
    "\n",
    "                # get the emission count of the (pos,word) tuple \n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    # keep track of the POS with the largest count\n",
    "                    if count > count_final: # complete this line\n",
    "\n",
    "                        # update the final count (largest count)\n",
    "                        count_final = count\n",
    "\n",
    "                        # update the final POS\n",
    "                        pos_final = pos\n",
    "\n",
    "            # If the final POS (with the largest count) matches the true POS:\n",
    "            if pos_final ==true_label : # complete this line\n",
    "                \n",
    "                # Update the number of correct predictions\n",
    "                num_correct += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1bcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.797116074\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_transition_matrix\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    ### START CODE HERE (Return instances of 'None' with your code) ### \n",
    "    \n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "        \n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionaory\n",
    "            if transition_counts: #complete this line\n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "                \n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and number of total tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[10:15,10:15], index=states[10:15], columns = states[10:15] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a127ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for\n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # Get a set of all (POS, word) tuples \n",
    "    # from the keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Go through each row (POS tags)\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        # Go through each column (words)\n",
    "        for j in range(num_words): # complete this line\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0\n",
    "                    \n",
    "            # Define the (POS tag, word) tuple for this row and column\n",
    "            key =  (all_tags[i],vocab[j])\n",
    "\n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emission_counts.keys(): # complete this line\n",
    "        \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts[key]\n",
    "                \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "                \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['sera']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b26def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['sera','nouveau','jour', 'entre', 'architecture']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "print(cols)\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =[ 'ADJWH', 'ADV', 'ADVWH', 'CC']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "print(rows)\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: initialize\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    # POS tags in the rows, number of words in the corpus as the columns\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    # POS tags in the rows, number of words in the corpus as columns\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Go through each of the POS tags\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        # Handle the special case when the transition from start token to POS tag i is zero\n",
    "        if A[s_idx,i] == 0: # complete this line\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
    "            best_probs[i,0] = float('-inf')\n",
    "        \n",
    "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
    "        else:\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0\n",
    "            # Check the formula in the instructions above\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
    "                        \n",
    "    ### END CODE HERE ### \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce623184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # complete this line\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags): # complete this line\n",
    "            \n",
    "                # Calculate the probability = \n",
    "                # best probs of POS tag k, previous word i-1 + \n",
    "                # log(prob of transition from POS k to POS j) + \n",
    "                # log(prob that emission of POS j is word i)\n",
    "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
    "\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i: # complete this line\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26289eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    ## Step 1 ##\n",
    "    \n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) \n",
    "    # with highest probability for the last word\n",
    "    for k in range(num_tags): # complete this line\n",
    "\n",
    "        # If the probability of POS tag at row k \n",
    "        # is better than the previosly best probability for the last word:\n",
    "        if best_probs[k,-1]>best_prob_for_last_word: # complete this line\n",
    "            \n",
    "            # Store the new best probability for the lsat word\n",
    "            best_prob_for_last_word = best_probs[k,-1]\n",
    "    \n",
    "            # Store the unique integer ID of the POS tag\n",
    "            # which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag\n",
    "    # from its unique integer ID into the string representation\n",
    "    # using the 'states' dictionary\n",
    "    # store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[k]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
    "        \n",
    "        # Retrieve the unique integer ID of\n",
    "        # the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
    "        \n",
    "        # In best_paths, go to the row representing the POS tag of word i\n",
    "        # and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' dictionary, \n",
    "        # where the key is the unique integer ID of the POS tag,\n",
    "        # and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[pos_tag_for_word_i]\n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b70246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split('_')\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple)!=2: # complete this line\n",
    "            continue \n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag: # complete this line\n",
    "            \n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36560742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
